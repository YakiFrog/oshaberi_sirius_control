#!/usr/bin/env python3
"""
LM Studio ãƒãƒ£ãƒƒãƒˆãƒ†ã‚¹ãƒˆãƒ—ãƒ­ã‚°ãƒ©ãƒ ï¼ˆéåŒæœŸç‰ˆï¼‰
ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã‹ã‚‰å…¥åŠ›ã—ã¦AIå¿œç­”ã‚’è¡¨ç¤º

éåŒæœŸã®ãƒ¡ãƒªãƒƒãƒˆ:
- ãƒ¬ã‚¹ãƒãƒ³ã‚¹å¾…ã¡ä¸­ã«ä»–ã®ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œå¯èƒ½ï¼ˆä¾‹: ä¸¦è¡Œãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚„UIæ›´æ–°ï¼‰
- è¤‡æ•°ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’åŒæ™‚ã«å‡¦ç†ã§ãã‚‹ãŸã‚ã€é«˜è² è·æ™‚ã«åŠ¹ç‡çš„
- GUIã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã§UIã®ãƒ•ãƒªãƒ¼ã‚ºã‚’é˜²ã’ã‚‹
- ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯é…å»¶ãŒå¤§ãã„å ´åˆã«å…¨ä½“ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãŒå‘ä¸Š
"""

import aiohttp
import asyncio
import time

# åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ãƒªã‚¹ãƒˆ
MODELS = {
    "1": {"name": "mistralai/magistral-small-2509", "description": "Mistral Smallãƒ¢ãƒ‡ãƒ«"},
    "2": {"name": "openai/gpt-oss-20b", "description": "OpenAI GPT OSS 20Bãƒ¢ãƒ‡ãƒ«"}
}

def select_model():
    """èµ·å‹•æ™‚ã«ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ"""
    print("åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«:")
    for key, model in MODELS.items():
        print(f"{key}: {model['description']} ({model['name']})")
    
    while True:
        choice = input("ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„ (1 ã¾ãŸã¯ 2): ").strip()
        if choice in MODELS:
            selected_model = MODELS[choice]["name"]
            print(f"é¸æŠã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«: {MODELS[choice]['description']}")
            return selected_model
        else:
            print("ç„¡åŠ¹ãªé¸æŠã§ã™ã€‚1 ã¾ãŸã¯ 2 ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")

async def chat_with_ai_async(user_message, model):
    url = "http://localhost:1234/v1/chat/completions"
    payload = {
        "model": model,
        "messages": [
            {"role": "system", "content": "ã‚ãªãŸã¯è¦ªåˆ‡ã§å½¹ç«‹ã¤AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚æ—¥æœ¬èªã§ç­”ãˆã¦ãã ã•ã„ã€‚"},
            {"role": "user", "content": user_message}
        ],
        "temperature": 0.7,
        "max_tokens": -1,
        "stream": False
    }
    
    start_time = time.time()
    async with aiohttp.ClientSession() as session:
        async with session.post(url, json=payload) as response:
            response.raise_for_status()
            result = await response.json()
            end_time = time.time()
            elapsed_time = end_time - start_time
            ai_message = result["choices"][0]["message"]["content"]
            return ai_message, elapsed_time

async def main():
    print("ğŸ¤– LM Studio ãƒãƒ£ãƒƒãƒˆãƒ†ã‚¹ãƒˆï¼ˆéåŒæœŸç‰ˆï¼‰")
    print("çµ‚äº†ã™ã‚‹ã«ã¯ 'quit' ã¾ãŸã¯ 'exit' ã¨å…¥åŠ›ã—ã¦ãã ã•ã„")
    print("-" * 50)
    
    # ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ
    selected_model = select_model()
    
    try:
        while True:
            user_input = input("ã‚ãªãŸ: ").strip()  # å…¥åŠ›ã¯åŒæœŸã®ã¾ã¾
            if user_input.lower() in ['quit', 'exit', 'q']:
                print("ãƒãƒ£ãƒƒãƒˆã‚’çµ‚äº†ã—ã¾ã™ã€‚")
                break
            if not user_input:
                continue
            
            print("AI: ", end="", flush=True)
            ai_response, elapsed_time = await chat_with_ai_async(user_input, selected_model)
            print(ai_response)
            print(f"å¿œç­”æ™‚é–“: {elapsed_time:.2f}ç§’")
            print("-" * 50)
    except KeyboardInterrupt:
        print("\nãƒãƒ£ãƒƒãƒˆã‚’çµ‚äº†ã—ã¾ã™ã€‚")

if __name__ == "__main__":
    asyncio.run(main())
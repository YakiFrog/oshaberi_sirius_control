#!/usr/bin/env python3
"""
ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ Faster WhisperéŸ³å£°èªè­˜ãƒ—ãƒ­ã‚°ãƒ©ãƒ 
faster_whisper_test.pyã®æœ€é©åŒ–è¨­å®šã‚’é©ç”¨ã—ãŸãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç‰ˆ
"""

import pyaudio
import numpy as np
import time
import threading
import queue
from faster_whisper import WhisperModel

# =============================================================================
# ğŸš€ é«˜é€ŸåŒ–è¨­å®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆfaster_whisper_test.pyã‹ã‚‰ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼‰
# =============================================================================

# ãƒ¢ãƒ‡ãƒ«è¨­å®šï¼ˆé«˜é€ŸåŒ–ã®åŸºç›¤ï¼‰
MODEL_CONFIG = {
    "model_size": "small",         # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç”¨ã«smallãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨
    "device": "cpu",
    "compute_type": "int8",
    "cpu_threads": 6,              # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç”¨ã«é©åº¦ãªã‚¹ãƒ¬ãƒƒãƒ‰æ•°
    "num_workers": 1
}

# èªè­˜ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆé€Ÿåº¦é‡è¦–ã®ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ï¼‰
TRANSCRIBE_CONFIG = {
    "language": "ja",
    "beam_size": 1,                # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç”¨ã«greedy decoding
    "temperature": 0.0,
    "compression_ratio_threshold": 2.0,
    "log_prob_threshold": -0.8,
    "no_speech_threshold": 0.4,
    "condition_on_previous_text": True,   # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç”¨ã«æ–‡è„ˆã‚’ç¶­æŒ
    "initial_prompt": "ä»¥ä¸‹ã¯æ—¥æœ¬èªã®éŸ³å£°ã§ã™ã€‚",
    "word_timestamps": False,
    "vad_filter": True,
    "vad_parameters": {
        "min_silence_duration_ms": 500,   # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç”¨ã«çŸ­ã‚ã«
        "speech_pad_ms": 30,
        "threshold": 0.5
    }
}

# ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è¨­å®š
REALTIME_CONFIG = {
    "chunk": 1024,
    "format": pyaudio.paInt16,
    "channels": 1,
    "rate": 16000,
    "buffer_seconds": 1.0,         # 1ç§’åˆ†ã®ãƒãƒƒãƒ•ã‚¡
    "overlap_seconds": 0.2,        # 0.2ç§’ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—
    "processing_interval": 0.5     # 0.5ç§’ã”ã¨ã«å‡¦ç†
}

# =============================================================================
# ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ éŸ³å£°èªè­˜ã‚¯ãƒ©ã‚¹
# =============================================================================

class RealtimeWhisper:
    def __init__(self):
        # ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
        print("ğŸš€ Faster Whisperãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
        self.model = WhisperModel(
            MODEL_CONFIG["model_size"],
            device=MODEL_CONFIG["device"],
            compute_type=MODEL_CONFIG["compute_type"],
            cpu_threads=MODEL_CONFIG["cpu_threads"],
            num_workers=MODEL_CONFIG["num_workers"]
        )
        print("âœ… ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å®Œäº†")

        # éŸ³å£°è¨­å®š
        self.audio = pyaudio.PyAudio()
        self.audio_queue = queue.Queue()
        self.is_running = False
        self.last_processed_time = 0

        # ãƒãƒƒãƒ•ã‚¡ç®¡ç†
        self.audio_buffer = np.array([], dtype=np.int16)
        self.buffer_lock = threading.Lock()

    def start_realtime_recognition(self):
        """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ èªè­˜ã‚’é–‹å§‹"""
        print("ğŸ¤ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ éŸ³å£°èªè­˜ã‚’é–‹å§‹ã—ã¾ã™...")
        print("ğŸ’¡ è©±ã—ã‹ã‘ã‚‹ã¨è‡ªå‹•ã§èªè­˜ã•ã‚Œã¾ã™ï¼ˆCtrl+Cã§çµ‚äº†ï¼‰")

        # éŸ³å£°ã‚¹ãƒˆãƒªãƒ¼ãƒ é–‹å§‹
        self.stream = self.audio.open(
            format=REALTIME_CONFIG["format"],
            channels=REALTIME_CONFIG["channels"],
            rate=REALTIME_CONFIG["rate"],
            input=True,
            frames_per_buffer=REALTIME_CONFIG["chunk"]
        )

        self.is_running = True

        # éŒ²éŸ³ã‚¹ãƒ¬ãƒƒãƒ‰é–‹å§‹
        recording_thread = threading.Thread(target=self._recording_worker, daemon=True)
        recording_thread.start()

        # å‡¦ç†ã‚¹ãƒ¬ãƒƒãƒ‰é–‹å§‹
        processing_thread = threading.Thread(target=self._processing_worker, daemon=True)
        processing_thread.start()

        try:
            # ãƒ¡ã‚¤ãƒ³ã‚¹ãƒ¬ãƒƒãƒ‰ã¯å¾…æ©Ÿ
            while self.is_running:
                time.sleep(0.1)
        except KeyboardInterrupt:
            print("\nğŸ›‘ çµ‚äº†ã—ã¾ã™...")
            self.stop_realtime_recognition()

    def stop_realtime_recognition(self):
        """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ èªè­˜ã‚’åœæ­¢"""
        self.is_running = False

        if hasattr(self, 'stream'):
            self.stream.stop_stream()
            self.stream.close()

        self.audio.terminate()
        print("âœ… ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ èªè­˜ã‚’çµ‚äº†ã—ã¾ã—ãŸ")

    def _recording_worker(self):
        """éŸ³å£°éŒ²éŸ³ãƒ¯ãƒ¼ã‚«ãƒ¼"""
        print("ğŸ™ï¸  éŒ²éŸ³ã‚¹ãƒ¬ãƒƒãƒ‰é–‹å§‹")

        while self.is_running:
            try:
                # éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
                data = self.stream.read(REALTIME_CONFIG["chunk"], exception_on_overflow=False)
                audio_chunk = np.frombuffer(data, dtype=np.int16)

                # ãƒãƒƒãƒ•ã‚¡ã«è¿½åŠ 
                with self.buffer_lock:
                    self.audio_buffer = np.concatenate([self.audio_buffer, audio_chunk])

                # éŸ³é‡ãƒ¬ãƒ™ãƒ«è¡¨ç¤ºï¼ˆ100msã”ã¨ï¼‰
                current_time = time.time()
                if int(current_time * 10) % 10 == 0:  # 100msã”ã¨
                    volume = self._calculate_volume(audio_chunk)
                    print(f"ğŸµ éŸ³é‡: {volume:.0f}", end='\r')

            except Exception as e:
                print(f"âŒ éŒ²éŸ³ã‚¨ãƒ©ãƒ¼: {e}")
                break

        print("\nğŸ“¥ éŒ²éŸ³ã‚¹ãƒ¬ãƒƒãƒ‰çµ‚äº†")

    def _processing_worker(self):
        """éŸ³å£°å‡¦ç†ãƒ¯ãƒ¼ã‚«ãƒ¼"""
        print("ğŸ§  å‡¦ç†ã‚¹ãƒ¬ãƒƒãƒ‰é–‹å§‹")

        while self.is_running:
            current_time = time.time()

            # å‡¦ç†é–“éš”ãƒã‚§ãƒƒã‚¯
            if current_time - self.last_processed_time < REALTIME_CONFIG["processing_interval"]:
                time.sleep(0.05)  # çŸ­ã„å¾…æ©Ÿ
                continue

            # ååˆ†ãªãƒãƒƒãƒ•ã‚¡ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            with self.buffer_lock:
                buffer_duration = len(self.audio_buffer) / REALTIME_CONFIG["rate"]

                if buffer_duration < REALTIME_CONFIG["buffer_seconds"]:
                    time.sleep(0.05)  # ãƒãƒƒãƒ•ã‚¡ãŒè¶³ã‚Šãªã„å ´åˆã¯å¾…æ©Ÿ
                    continue

                # å‡¦ç†ã™ã‚‹éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºï¼ˆã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—è€ƒæ…®ï¼‰
                process_samples = int(REALTIME_CONFIG["buffer_seconds"] * REALTIME_CONFIG["rate"])
                overlap_samples = int(REALTIME_CONFIG["overlap_seconds"] * REALTIME_CONFIG["rate"])

                # æœ€æ–°ã®ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†
                audio_to_process = self.audio_buffer[-process_samples:]

                # ãƒãƒƒãƒ•ã‚¡ã‚’ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—åˆ†æ®‹ã—ã¦æ›´æ–°
                keep_samples = overlap_samples
                if len(self.audio_buffer) > keep_samples:
                    self.audio_buffer = self.audio_buffer[-keep_samples:]

            # éŸ³å£°èªè­˜å‡¦ç†
            try:
                self._process_audio_chunk(audio_to_process)
                self.last_processed_time = current_time

            except Exception as e:
                print(f"âŒ å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")

        print("\nğŸ”„ å‡¦ç†ã‚¹ãƒ¬ãƒƒãƒ‰çµ‚äº†")

    def _process_audio_chunk(self, audio_chunk):
        """éŸ³å£°ãƒãƒ£ãƒ³ã‚¯ã‚’å‡¦ç†"""
        # NumPyé…åˆ—ã‚’float32ã«å¤‰æ›
        audio_np = audio_chunk.astype(np.float32) / 32768.0

        # ç„¡éŸ³ãƒã‚§ãƒƒã‚¯
        volume = self._calculate_volume(audio_chunk)
        if volume < 100:  # éŸ³é‡ãŒå°ã•ã™ãã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
            return

        # Whisperã§èªè­˜
        segments, info = self.model.transcribe(
            audio_np,
            language=TRANSCRIBE_CONFIG["language"],
            beam_size=TRANSCRIBE_CONFIG["beam_size"],
            temperature=TRANSCRIBE_CONFIG["temperature"],
            compression_ratio_threshold=TRANSCRIBE_CONFIG["compression_ratio_threshold"],
            log_prob_threshold=TRANSCRIBE_CONFIG["log_prob_threshold"],
            no_speech_threshold=TRANSCRIBE_CONFIG["no_speech_threshold"],
            condition_on_previous_text=TRANSCRIBE_CONFIG["condition_on_previous_text"],
            initial_prompt=TRANSCRIBE_CONFIG["initial_prompt"],
            word_timestamps=TRANSCRIBE_CONFIG["word_timestamps"],
            vad_filter=TRANSCRIBE_CONFIG["vad_filter"],
            vad_parameters=TRANSCRIBE_CONFIG["vad_parameters"]
        )

        # çµæœã‚’è¡¨ç¤º
        segments_list = list(segments)
        if segments_list:
            text = "".join(segment.text for segment in segments_list).strip()
            if text:  # ç©ºã§ãªã„ãƒ†ã‚­ã‚¹ãƒˆã®ã¿è¡¨ç¤º
                confidence = self._calculate_simple_confidence(segments_list, info)
                duration = len(audio_chunk) / REALTIME_CONFIG["rate"]
                print(f"\nğŸ¯ [{duration:.1f}s] {text} (ç¢ºä¿¡åº¦: {confidence:.1f}%)")

    def _calculate_volume(self, audio_chunk):
        """éŸ³é‡ã‚’è¨ˆç®—"""
        if len(audio_chunk) == 0:
            return 0
        return np.sqrt(np.mean(audio_chunk.astype(np.float64)**2))

    def _calculate_simple_confidence(self, segments, info):
        """ç°¡æ˜“çš„ãªä¿¡é ¼åº¦è¨ˆç®—"""
        try:
            if hasattr(info, 'language_probability') and info.language_probability:
                return info.language_probability * 100

            # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®å¹³å‡ç¢ºç‡ã‚’ä½¿ç”¨
            confidences = []
            for segment in segments:
                if hasattr(segment, 'avg_logprob') and segment.avg_logprob is not None:
                    # å¯¾æ•°ç¢ºç‡ã‚’ãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸ã«å¤‰æ›
                    confidence = min(100.0, max(0.0, (segment.avg_logprob + 5.0) / 5.0 * 100))
                    confidences.append(confidence)

            return sum(confidences) / len(confidences) if confidences else 50.0

        except:
            return 50.0

# =============================================================================
# ãƒ¡ã‚¤ãƒ³é–¢æ•°
# =============================================================================

def main():
    print("ğŸ¤ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ  Faster Whisper éŸ³å£°èªè­˜")
    print("=" * 50)
    print(f"ğŸ“Š ãƒ¢ãƒ‡ãƒ«: {MODEL_CONFIG['model_size']} ({MODEL_CONFIG['compute_type']})")
    print(f"âš¡ CPUã‚¹ãƒ¬ãƒƒãƒ‰: {MODEL_CONFIG['cpu_threads']}")
    print(f"â±ï¸  å‡¦ç†é–“éš”: {REALTIME_CONFIG['processing_interval']}ç§’")
    print(f"ğŸµ ãƒãƒƒãƒ•ã‚¡: {REALTIME_CONFIG['buffer_seconds']}ç§’")
    print("=" * 50)

    # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ èªè­˜ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä½œæˆ
    recognizer = RealtimeWhisper()

    try:
        # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ èªè­˜é–‹å§‹
        recognizer.start_realtime_recognition()

    except KeyboardInterrupt:
        print("\nğŸ›‘ ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’çµ‚äº†ã—ã¾ã™")
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
    finally:
        recognizer.stop_realtime_recognition()

if __name__ == "__main__":
    main()
